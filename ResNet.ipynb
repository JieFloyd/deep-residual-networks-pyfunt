{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnet.data.data_utils import get_CIFAR10_data\n",
    "from nnet.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from nnet.solver import Solver\n",
    "from nnet.data.data_utils import load_CIFAR10\n",
    "from nnet.res_net import ResNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-2, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ 0.00894057  0.02568426 -0.00970689]\n",
      "  stds:  [ 0.99063808  1.00463239  0.98970638]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [ -2.33146835e-17  -8.10129741e-17  -2.00228722e-17]\n",
      "  std:  [ 1.  1.  1.]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 50, 3, 10, 10\n",
    "X = np.random.randn(N, D1, D2, D3)\n",
    "#W1 = np.random.randn(D1, D2)\n",
    "#W2 = np.random.randn(D2, D3)\n",
    "#a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print 'Before batch normalization:'\n",
    "print '  means: ', X.mean(axis=(0, 2, 3))\n",
    "print '  stds: ', X.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print 'After batch normalization (gamma=1, beta=0)'\n",
    "a_norm, _ = spatial_batchnorm_forward(X, np.ones(D1), np.zeros(D1), {'mode': 'train'})\n",
    "print '  mean: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  std: ', a_norm.std(axis=(0, 2, 3))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = spatial_batchnorm_forward(X, gamma, beta, {'mode': 'train'})\n",
    "print 'After batch normalization (nontrivial gamma, beta)'\n",
    "print '  means: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  stds: ', a_norm.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [ 0.21431437  0.26381058 -0.11471238]\n",
      "  stds:  [ 1.11203999  1.04931796  0.97440943]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, D1, D2, D3 = 20, 3, 1, 1\n",
    "#W1 = np.random.randn(D1, D2)\n",
    "#W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D1)\n",
    "beta = np.zeros(D1)\n",
    "for t in xrange(100):\n",
    "  X = np.random.randn(N, D1, D2, D3)\n",
    "  \n",
    "  spatial_batchnorm_forward(X, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1, D2, D3)\n",
    "a_norm, _ = spatial_batchnorm_forward(X, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print 'After batch normalization (test-time):'\n",
    "print '  means: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  stds: ', a_norm.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.15957511654e-05\n",
      "dgamma error:  2.91997839797e-12\n",
      "dbeta error:  3.60408390141e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D, H, W = 20, 3, 10, 10\n",
    "x = 5 * np.random.randn(N, D, H, W) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "#dmean_num = np.sum(dx_num, axis = (0,2,3))*N\n",
    "#dmean_num = np.sum(dx_num, axis = 0)*N\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "#print 'dmean error: ', rel_error(dmean_num, dmean), dmean, dmean_num\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dgamma error: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta error: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Nets with Batch Normalization\n",
    "Now that you have a working implementation for batch normalization, go back to your `FullyConnectedNet` in the file `cs2312n/classifiers/fc_net.py`. Modify your implementation to add batch normalization.\n",
    "\n",
    "Concretely, when the flag `use_batchnorm` is `True` in the constructor, you should insert a batch normalization layer before each ReLU nonlinearity. The outputs from the last layer of the network should not be normalized. Once you are done, run the following to gradient-check your implementation.\n",
    "\n",
    "HINT: You might find it useful to define an additional helper layer similar to those in the file `cs231n/layer_utils.py`. If you decide to do so, do it in the file `cs231n/classifiers/fc_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "[2, 2, 2, 4, 4, 8, 8]\n",
      "Initial loss:  2.3309981823\n",
      "W1 relative error: 1.00e+00\n",
      "W2 relative error: 1.00e+00\n",
      "W3 relative error: 1.00e+00\n",
      "W4 relative error: 1.00e+00\n",
      "W5 relative error: 1.00e+00\n",
      "W6 relative error: 1.00e+00\n",
      "W7 relative error: 1.00e+00\n",
      "W9 relative error: 1.00e+00\n",
      "b1 relative error: 1.00e+00\n",
      "b2 relative error: 1.00e+00\n",
      "b3 relative error: 1.00e+00\n",
      "b4 relative error: 1.00e+00\n",
      "b5 relative error: 1.00e+00\n",
      "b6 relative error: 1.00e+00\n",
      "b7 relative error: 1.00e+00\n",
      "b9 relative error: 1.00e+00\n",
      "beta1 relative error: 5.16e-01\n",
      "beta2 relative error: 9.74e-01\n",
      "beta3 relative error: 6.72e-01\n",
      "beta4 relative error: 1.00e+00\n",
      "beta5 relative error: 6.49e-01\n",
      "beta6 relative error: 1.00e+00\n",
      "beta7 relative error: 1.00e+00\n",
      "gamma1 relative error: 1.00e+00\n",
      "gamma2 relative error: 9.78e-01\n",
      "gamma3 relative error: 1.00e+00\n",
      "gamma4 relative error: 9.80e-01\n",
      "gamma5 relative error: 1.00e+00\n",
      "gamma6 relative error: 1.00e+00\n",
      "gamma7 relative error: 1.00e+00\n",
      "\n",
      "Running check with reg =  3.14\n",
      "[2, 2, 2, 4, 4, 8, 8]\n",
      "Initial loss:  3.96292161942\n",
      "W1 relative error: 1.00e+00\n",
      "W2 relative error: 1.00e+00\n",
      "W3 relative error: 1.00e+00\n",
      "W4 relative error: 1.00e+00\n",
      "W5 relative error: 1.00e+00\n",
      "W6 relative error: 1.00e+00\n",
      "W7 relative error: 1.00e+00\n",
      "W9 relative error: 1.00e+00\n",
      "b1 relative error: 1.00e+00\n",
      "b2 relative error: 1.00e+00\n",
      "b3 relative error: 1.00e+00\n",
      "b4 relative error: 1.00e+00\n",
      "b5 relative error: 1.00e+00\n",
      "b6 relative error: 1.00e+00\n",
      "b7 relative error: 1.00e+00\n",
      "b9 relative error: 1.00e+00\n",
      "beta1 relative error: 1.00e+00\n",
      "beta2 relative error: 1.00e+00\n",
      "beta3 relative error: 1.00e+00\n",
      "beta4 relative error: 1.00e+00\n",
      "beta5 relative error: 1.00e+00\n",
      "beta6 relative error: 1.00e+00\n",
      "beta7 relative error: 1.00e+00\n",
      "gamma1 relative error: 2.76e-01\n",
      "gamma2 relative error: 9.33e-01\n",
      "gamma3 relative error: 1.00e+00\n",
      "gamma4 relative error: 1.00e+00\n",
      "gamma5 relative error: 1.00e+00\n",
      "gamma6 relative error: 1.00e+00\n",
      "gamma7 relative error: 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 1, 3, 20, 30, 10\n",
    "#X = np.random.randn(N, D)\n",
    "#y = np.random.randint(C, size=(N,))\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print 'Running check with reg = ', reg\n",
    "\n",
    "  model = ResNet(n_size=1, num_starting_filters=2)\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print 'Initial loss: ', loss\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-6)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))\n",
    "  if reg == 0: print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 16, 16, 32, 32, 64, 64]\n",
      "Training for 20 epochs, or 100 iterations.\n",
      "2016-03-05 22:07:19.119856: Step 1, loss: 2.898 train acc: 0.160; val_acc: 0.104 (0.32 sec/batch)\n",
      "2016-03-05 22:07:26.312264: Step 2, loss: 2.388 train acc: 0.140; val_acc: 0.116 (0.32 sec/batch)\n",
      "2016-03-05 22:07:33.543391: Step 3, loss: 2.382 train acc: 0.150; val_acc: 0.112 (0.32 sec/batch)\n",
      "2016-03-05 22:07:40.774980: Step 4, loss: 2.943 train acc: 0.180; val_acc: 0.127 (0.32 sec/batch)\n",
      "2016-03-05 22:07:47.943591: Step 5, loss: 3.051 train acc: 0.210; val_acc: 0.143 (0.31 sec/batch)\n",
      "*Epoch 0 / 20 Ended: best_val_acc: 0.143000\n",
      "2016-03-05 22:07:55.160426: Step 6, loss: 2.723 train acc: 0.240; val_acc: 0.146 (0.32 sec/batch)\n",
      "2016-03-05 22:08:02.449930: Step 7, loss: 3.289 train acc: 0.130; val_acc: 0.108 (0.31 sec/batch)\n",
      "2016-03-05 22:08:09.794936: Step 8, loss: 2.294 train acc: 0.180; val_acc: 0.130 (0.31 sec/batch)\n",
      "2016-03-05 22:08:17.048611: Step 9, loss: 3.307 train acc: 0.240; val_acc: 0.167 (0.31 sec/batch)\n",
      "2016-03-05 22:08:24.349016: Step 10, loss: 2.016 train acc: 0.170; val_acc: 0.133 (0.31 sec/batch)\n",
      "*Epoch 1 / 20 Ended: best_val_acc: 0.167000\n",
      "2016-03-05 22:08:31.967500: Step 11, loss: 2.528 train acc: 0.290; val_acc: 0.153 (0.32 sec/batch)\n",
      "2016-03-05 22:08:39.648308: Step 12, loss: 2.917 train acc: 0.350; val_acc: 0.175 (0.32 sec/batch)\n",
      "2016-03-05 22:08:47.246895: Step 13, loss: 2.311 train acc: 0.290; val_acc: 0.185 (0.33 sec/batch)\n",
      "2016-03-05 22:08:55.015933: Step 14, loss: 2.502 train acc: 0.290; val_acc: 0.152 (0.32 sec/batch)\n",
      "2016-03-05 22:09:02.537321: Step 15, loss: 2.466 train acc: 0.250; val_acc: 0.148 (0.34 sec/batch)\n",
      "*Epoch 2 / 20 Ended: best_val_acc: 0.185000\n",
      "2016-03-05 22:09:09.929134: Step 16, loss: 2.902 train acc: 0.140; val_acc: 0.089 (0.33 sec/batch)\n",
      "2016-03-05 22:09:17.409458: Step 17, loss: 3.097 train acc: 0.300; val_acc: 0.192 (0.32 sec/batch)\n",
      "2016-03-05 22:09:24.820741: Step 18, loss: 2.428 train acc: 0.270; val_acc: 0.199 (0.31 sec/batch)\n",
      "2016-03-05 22:09:32.291482: Step 19, loss: 2.087 train acc: 0.220; val_acc: 0.199 (0.34 sec/batch)\n",
      "2016-03-05 22:09:39.814239: Step 20, loss: 2.561 train acc: 0.350; val_acc: 0.210 (0.32 sec/batch)\n",
      "*Epoch 3 / 20 Ended: best_val_acc: 0.210000\n",
      "2016-03-05 22:09:47.537400: Step 21, loss: 1.915 train acc: 0.290; val_acc: 0.163 (0.34 sec/batch)\n",
      "2016-03-05 22:09:55.263376: Step 22, loss: 2.502 train acc: 0.350; val_acc: 0.182 (0.35 sec/batch)\n",
      "2016-03-05 22:10:02.961635: Step 23, loss: 2.460 train acc: 0.380; val_acc: 0.185 (0.34 sec/batch)\n",
      "2016-03-05 22:10:10.649598: Step 24, loss: 2.277 train acc: 0.340; val_acc: 0.189 (0.34 sec/batch)\n",
      "2016-03-05 22:10:18.321825: Step 25, loss: 1.661 train acc: 0.300; val_acc: 0.176 (0.34 sec/batch)\n",
      "*Epoch 4 / 20 Ended: best_val_acc: 0.210000\n",
      "2016-03-05 22:10:25.984735: Step 26, loss: 2.483 train acc: 0.340; val_acc: 0.153 (0.34 sec/batch)\n",
      "2016-03-05 22:10:33.689321: Step 27, loss: 2.138 train acc: 0.270; val_acc: 0.182 (0.35 sec/batch)\n",
      "2016-03-05 22:10:41.351677: Step 28, loss: 2.066 train acc: 0.360; val_acc: 0.160 (0.34 sec/batch)\n",
      "2016-03-05 22:10:48.994033: Step 29, loss: 2.355 train acc: 0.400; val_acc: 0.224 (0.34 sec/batch)\n",
      "2016-03-05 22:10:56.666474: Step 30, loss: 2.170 train acc: 0.390; val_acc: 0.200 (0.32 sec/batch)\n",
      "*Epoch 5 / 20 Ended: best_val_acc: 0.224000\n",
      "2016-03-05 22:11:04.344466: Step 31, loss: 1.532 train acc: 0.360; val_acc: 0.189 (0.34 sec/batch)\n",
      "2016-03-05 22:11:12.029334: Step 32, loss: 1.521 train acc: 0.330; val_acc: 0.187 (0.33 sec/batch)\n",
      "2016-03-05 22:11:19.725332: Step 33, loss: 1.891 train acc: 0.370; val_acc: 0.179 (0.32 sec/batch)\n",
      "2016-03-05 22:11:27.402700: Step 34, loss: 1.656 train acc: 0.240; val_acc: 0.142 (0.33 sec/batch)\n",
      "2016-03-05 22:11:35.048529: Step 35, loss: 1.765 train acc: 0.320; val_acc: 0.165 (0.32 sec/batch)\n",
      "*Epoch 6 / 20 Ended: best_val_acc: 0.224000\n",
      "2016-03-05 22:11:42.605546: Step 36, loss: 1.843 train acc: 0.360; val_acc: 0.168 (0.32 sec/batch)\n",
      "2016-03-05 22:11:49.841905: Step 37, loss: 1.604 train acc: 0.310; val_acc: 0.141 (0.33 sec/batch)\n",
      "2016-03-05 22:11:57.053608: Step 38, loss: 1.811 train acc: 0.420; val_acc: 0.176 (0.31 sec/batch)\n",
      "2016-03-05 22:12:04.279957: Step 39, loss: 2.029 train acc: 0.410; val_acc: 0.172 (0.30 sec/batch)\n",
      "2016-03-05 22:12:12.033661: Step 40, loss: 1.825 train acc: 0.370; val_acc: 0.176 (0.32 sec/batch)\n",
      "*Epoch 7 / 20 Ended: best_val_acc: 0.224000\n",
      "2016-03-05 22:12:19.340804: Step 41, loss: 1.675 train acc: 0.410; val_acc: 0.173 (0.35 sec/batch)\n",
      "2016-03-05 22:12:26.557300: Step 42, loss: 2.144 train acc: 0.390; val_acc: 0.191 (0.31 sec/batch)\n",
      "2016-03-05 22:12:33.787885: Step 43, loss: 1.990 train acc: 0.340; val_acc: 0.158 (0.32 sec/batch)\n",
      "2016-03-05 22:12:40.998146: Step 44, loss: 1.833 train acc: 0.300; val_acc: 0.143 (0.31 sec/batch)\n",
      "2016-03-05 22:12:48.193255: Step 45, loss: 2.149 train acc: 0.460; val_acc: 0.226 (0.32 sec/batch)\n",
      "*Epoch 8 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:12:55.349200: Step 46, loss: 1.912 train acc: 0.420; val_acc: 0.209 (0.31 sec/batch)\n",
      "2016-03-05 22:13:02.563478: Step 47, loss: 1.631 train acc: 0.390; val_acc: 0.200 (0.30 sec/batch)\n",
      "2016-03-05 22:13:09.726034: Step 48, loss: 1.737 train acc: 0.380; val_acc: 0.212 (0.32 sec/batch)\n",
      "2016-03-05 22:13:16.907992: Step 49, loss: 1.809 train acc: 0.290; val_acc: 0.177 (0.32 sec/batch)\n",
      "2016-03-05 22:13:24.118444: Step 50, loss: 1.407 train acc: 0.290; val_acc: 0.178 (0.31 sec/batch)\n",
      "*Epoch 9 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:13:31.320875: Step 51, loss: 1.856 train acc: 0.380; val_acc: 0.178 (0.31 sec/batch)\n",
      "2016-03-05 22:13:38.526855: Step 52, loss: 1.686 train acc: 0.340; val_acc: 0.183 (0.31 sec/batch)\n",
      "2016-03-05 22:13:45.704819: Step 53, loss: 1.771 train acc: 0.420; val_acc: 0.193 (0.32 sec/batch)\n",
      "2016-03-05 22:13:52.868382: Step 54, loss: 1.666 train acc: 0.430; val_acc: 0.198 (0.31 sec/batch)\n",
      "2016-03-05 22:14:00.045947: Step 55, loss: 1.860 train acc: 0.420; val_acc: 0.185 (0.30 sec/batch)\n",
      "*Epoch 10 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:14:07.202178: Step 56, loss: 1.689 train acc: 0.360; val_acc: 0.166 (0.32 sec/batch)\n",
      "2016-03-05 22:14:14.375668: Step 57, loss: 1.614 train acc: 0.450; val_acc: 0.204 (0.32 sec/batch)\n",
      "2016-03-05 22:14:21.529265: Step 58, loss: 1.797 train acc: 0.350; val_acc: 0.195 (0.33 sec/batch)\n",
      "2016-03-05 22:14:28.696990: Step 59, loss: 1.685 train acc: 0.360; val_acc: 0.207 (0.31 sec/batch)\n",
      "2016-03-05 22:14:35.915469: Step 60, loss: 1.893 train acc: 0.390; val_acc: 0.185 (0.31 sec/batch)\n",
      "*Epoch 11 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:14:43.167979: Step 61, loss: 1.502 train acc: 0.500; val_acc: 0.202 (0.30 sec/batch)\n",
      "2016-03-05 22:14:50.874378: Step 62, loss: 1.480 train acc: 0.410; val_acc: 0.225 (0.31 sec/batch)\n",
      "2016-03-05 22:14:58.595072: Step 63, loss: 2.051 train acc: 0.390; val_acc: 0.188 (0.31 sec/batch)\n",
      "2016-03-05 22:15:06.085542: Step 64, loss: 2.332 train acc: 0.390; val_acc: 0.184 (0.32 sec/batch)\n",
      "2016-03-05 22:15:13.794892: Step 65, loss: 1.569 train acc: 0.370; val_acc: 0.199 (0.32 sec/batch)\n",
      "*Epoch 12 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:15:21.130670: Step 66, loss: 1.586 train acc: 0.410; val_acc: 0.204 (0.35 sec/batch)\n",
      "2016-03-05 22:15:28.342302: Step 67, loss: 1.554 train acc: 0.420; val_acc: 0.205 (0.32 sec/batch)\n",
      "2016-03-05 22:15:35.503229: Step 68, loss: 1.556 train acc: 0.410; val_acc: 0.223 (0.32 sec/batch)\n",
      "2016-03-05 22:15:42.729022: Step 69, loss: 1.743 train acc: 0.500; val_acc: 0.209 (0.31 sec/batch)\n",
      "2016-03-05 22:15:49.969671: Step 70, loss: 1.728 train acc: 0.440; val_acc: 0.176 (0.31 sec/batch)\n",
      "*Epoch 13 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:15:57.337784: Step 71, loss: 1.565 train acc: 0.410; val_acc: 0.191 (0.34 sec/batch)\n",
      "2016-03-05 22:16:04.518403: Step 72, loss: 1.624 train acc: 0.490; val_acc: 0.205 (0.32 sec/batch)\n",
      "2016-03-05 22:16:11.836132: Step 73, loss: 1.789 train acc: 0.420; val_acc: 0.211 (0.33 sec/batch)\n",
      "2016-03-05 22:16:19.230733: Step 74, loss: 1.673 train acc: 0.430; val_acc: 0.204 (0.31 sec/batch)\n",
      "2016-03-05 22:16:26.615761: Step 75, loss: 1.751 train acc: 0.430; val_acc: 0.225 (0.31 sec/batch)\n",
      "*Epoch 14 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:16:33.868206: Step 76, loss: 1.968 train acc: 0.400; val_acc: 0.188 (0.31 sec/batch)\n",
      "2016-03-05 22:16:41.279349: Step 77, loss: 1.428 train acc: 0.430; val_acc: 0.183 (0.32 sec/batch)\n",
      "2016-03-05 22:16:48.481917: Step 78, loss: 1.721 train acc: 0.460; val_acc: 0.204 (0.32 sec/batch)\n",
      "2016-03-05 22:16:55.645422: Step 79, loss: 1.540 train acc: 0.380; val_acc: 0.185 (0.32 sec/batch)\n",
      "2016-03-05 22:17:02.801340: Step 80, loss: 1.733 train acc: 0.410; val_acc: 0.179 (0.33 sec/batch)\n",
      "*Epoch 15 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:17:10.471357: Step 81, loss: 1.558 train acc: 0.420; val_acc: 0.182 (0.33 sec/batch)\n",
      "2016-03-05 22:17:17.761745: Step 82, loss: 1.329 train acc: 0.440; val_acc: 0.207 (0.32 sec/batch)\n",
      "2016-03-05 22:17:25.074745: Step 83, loss: 1.816 train acc: 0.500; val_acc: 0.199 (0.33 sec/batch)\n",
      "2016-03-05 22:17:32.300932: Step 84, loss: 1.628 train acc: 0.480; val_acc: 0.186 (0.32 sec/batch)\n",
      "2016-03-05 22:17:39.506422: Step 85, loss: 1.549 train acc: 0.520; val_acc: 0.190 (0.32 sec/batch)\n",
      "*Epoch 16 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:17:46.891597: Step 86, loss: 1.433 train acc: 0.450; val_acc: 0.179 (0.31 sec/batch)\n",
      "2016-03-05 22:17:54.185389: Step 87, loss: 1.302 train acc: 0.490; val_acc: 0.194 (0.32 sec/batch)\n",
      "2016-03-05 22:18:01.622690: Step 88, loss: 1.499 train acc: 0.480; val_acc: 0.195 (0.32 sec/batch)\n",
      "2016-03-05 22:18:09.135645: Step 89, loss: 1.784 train acc: 0.490; val_acc: 0.217 (0.31 sec/batch)\n",
      "2016-03-05 22:18:16.454951: Step 90, loss: 1.483 train acc: 0.490; val_acc: 0.220 (0.31 sec/batch)\n",
      "*Epoch 17 / 20 Ended: best_val_acc: 0.226000\n",
      "2016-03-05 22:18:23.714879: Step 91, loss: 1.558 train acc: 0.520; val_acc: 0.231 (0.31 sec/batch)\n",
      "2016-03-05 22:18:31.111694: Step 92, loss: 1.441 train acc: 0.460; val_acc: 0.202 (0.31 sec/batch)\n",
      "2016-03-05 22:18:38.432247: Step 93, loss: 1.747 train acc: 0.530; val_acc: 0.223 (0.31 sec/batch)\n",
      "2016-03-05 22:18:45.654473: Step 94, loss: 1.350 train acc: 0.500; val_acc: 0.208 (0.32 sec/batch)\n",
      "2016-03-05 22:18:52.821491: Step 95, loss: 1.491 train acc: 0.510; val_acc: 0.212 (0.31 sec/batch)\n",
      "*Epoch 18 / 20 Ended: best_val_acc: 0.231000\n",
      "2016-03-05 22:19:00.012237: Step 96, loss: 1.744 train acc: 0.470; val_acc: 0.193 (0.31 sec/batch)\n",
      "2016-03-05 22:19:07.199228: Step 97, loss: 1.572 train acc: 0.520; val_acc: 0.204 (0.31 sec/batch)\n",
      "2016-03-05 22:19:14.348577: Step 98, loss: 1.214 train acc: 0.530; val_acc: 0.209 (0.30 sec/batch)\n",
      "2016-03-05 22:19:21.549836: Step 99, loss: 1.190 train acc: 0.430; val_acc: 0.200 (0.31 sec/batch)\n",
      "2016-03-05 22:19:28.738228: Step 100, loss: 1.511 train acc: 0.550; val_acc: 0.229 (0.32 sec/batch)\n",
      "*Epoch 19 / 20 Ended: best_val_acc: 0.231000\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ResNet(n_size=1)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=20,\n",
    "                update_rule='sgd_th',\n",
    "                optim_config={\n",
    "                  'learning_rate': .1, \n",
    "                  'nesterov': True,\n",
    "                  'momentum': .9,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
